{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "This notebook is an example of how to employ `ortho-svm` to perform classification tasks using **custom kernels** with Support Vector Machines. Althought `scikit-learn` already provides an API for this, `ortho-svm` makes it easier to do as it already has a couple of pre-defined kernels such as:\n",
    "\n",
    "- **Hermite kernel:** A special kernel defined by Hermite polynomials.\n",
    "- **Gegenbauer kernel:** A family of kernels defined by a parameter $\\alpha$ that generalizes most of the classic polynomials, e.g. Legendre and Chebyshev.\n",
    "- **Chebyshev kernel:** A particular instance of the Gegenbauer kernel when $\\alpha = 0$; the Chebyshev polynomials defined are the Chebyshev polynomials of the _first kind_.\n",
    "\n",
    "In this document, an example of a **classification** task will be performed by comparing the results obtained with those from the literature [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We will be performing classification for the first MONK's dataset [2] (monks-1) using the Gegenbauer kernel defined in `ortho-svm`.\n",
    "\n",
    "The **monks-1** dataset has the following properties:\n",
    "\n",
    "- 432 instances\n",
    "- 6 attributes\n",
    "- 2 classes\n",
    "\n",
    "The classification problem is quite simple, so we expect to get similar results to those reported. From the main references [1] we have the following special hyper-parameters for the Gegenbauer kernel:\n",
    "\n",
    "- $C = 20.96 \\pm 3.4$\n",
    "- $\\alpha = 0.38 \\pm 0.0$\n",
    "- $n = 2$\n",
    "\n",
    "These are the best values reported, and this will be considered the **ground truth** for the results obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "Here, we will be performing the following steps to achieve similar results to those in the literature:\n",
    "\n",
    "1. Pre-process the data, including loading, cleaning and creating a splitter for the following steps.\n",
    "2. Define a 5-fold cross validation the obtain the best $C$ parameter, the special parameters $n$ and $\\alpha$ will be taken for granted from those in the ground truth.\n",
    "3. Using the _training_ dataset, evaluate the 5-fold cross validation to find the best value, while simultaneously recording the **classification accuracy.**\n",
    "4. Finally, using all the hyper-parameters, test the trained Support Vector Machine on the _test_ dataset to evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from orthosvm.gramian import gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the _training_ dataset, which is contained in the same directory as this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(683, 10)\n"
     ]
    }
   ],
   "source": [
    "# We ignore the last column here because we don't need them, so we\n",
    "# define a list of columns to use\n",
    "# raw_data = np.loadtxt(\"monks-1.csv\", skiprows=1, delimiter=\",\")\n",
    "raw_data = np.loadtxt(\"breast.csv\", skiprows=1, delimiter=\",\")\n",
    "\n",
    "# From this, we extract the labels\n",
    "y = raw_data[:, -1]\n",
    "# We now extract the features\n",
    "xdata = raw_data[:, :-1]\n",
    "\n",
    "# Check number of instances and features\n",
    "print(xdata.shape)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(xdata, y, test_size=0.2, random_state=56)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Perform a 5-fold cross validation\n",
    "\n",
    "We now take the cleaned-up data and perform a grid search for the best possible value for $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to create our estimator, which is a special SVM for classification\n",
    "\n",
    "# We define the function that will compute the Grammian matrix\n",
    "# and we specify the hyper-parameters\n",
    "gram_matrix = gram.gram_matrix(kernel=\"hermite\", degree=8)\n",
    "\n",
    "# Next, we pass a dictionary of parameters, in this case the kernel, to the SVC\n",
    "# estimator and we create an object with it\n",
    "params = {\"kernel\": gram_matrix, \"C\": 8.0}\n",
    "# params = {\"kernel\": \"rbf\", \"C\": 19.34}\n",
    "svc = SVC(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we proceed to create the GridSearchCV\n",
    "\n",
    "# Create the grid to search\n",
    "c_grid = [{\"C\": [i for i in range(10, 101)]}]\n",
    "# Instantiate the grid search object\n",
    "# gcv = GridSearchCV(svc, c_grid, \"accuracy\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the model\n",
    "\n",
    "We now proceed to train the model using the set-up we have until now, and with the great `scikit-learn` API, after running `GridSearchCV` we obtain the best estimator for the data.\n",
    "\n",
    "**WARNING:** This might take a while, so if you have the processing resources to spare, enable the `n_jobs` parameter to make this easier for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=8.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale',\n",
       "    kernel=<function gram_matrix.<locals>.compute_gram_matrix at 0x7f18451a8050>,\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We proceed to fit the model to our data\n",
    "# we use the n_jobs parameter before to make up for the time spent here\n",
    "# gcv.fit(xdata, y)\n",
    "svc.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us compare the best value found for C\n",
    "# gcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are somewhat far from the estimated value for $C$ given in the main reference, but this will suffice for now. We can then use the best estimator found and try out the _test_ dataset with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Test the model\n",
    "\n",
    "We now proceed to test the model, we first load the testing dataset and then we proceed to find the classification accuracy with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6496350364963503\n"
     ]
    }
   ],
   "source": [
    "# We do the same as before to load the information, we can even re-use the same variables\n",
    "# raw_data = np.loadtxt(\"monks-1_test.csv\", skiprows=1, delimiter=\",\")\n",
    "\n",
    "# Again, we extract the labels\n",
    "# y = raw_data[:, -1]\n",
    "# y[y == 0] = -1\n",
    "\n",
    "# We now extract the features\n",
    "# xtest = raw_data[:, :-1]\n",
    "\n",
    "# Let us now test our model\n",
    "# y_predict = gcv.predict(xtest)\n",
    "# And print out a classification report\n",
    "# print(classification_report(ytest, y_predict))\n",
    "print(svc.score(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Luis Carlos Padierna, Martín Carpio, Alfonso Rojas-Domínguez, Héctor Puga, Héctor Fraire,\n",
    "    A novel formulation of orthogonal polynomial kernel functions for SVM classifiers: The Gegenbauer family,\n",
    "    Pattern Recognition,\n",
    "    Volume 84,\n",
    "    2018,\n",
    "    Pages 211-225,\n",
    "    ISSN 0031-3203,\n",
    "    https://doi.org/10.1016/j.patcog.2018.07.010.\n",
    "    (http://www.sciencedirect.com/science/article/pii/S0031320318302280)\n",
    "    Keywords: SVM classifier; Orthogonal polynomials; Gegenbauer kernel; Binary classification\n",
    "    \n",
    "2. MONK's dataset: https://archive.ics.uci.edu/ml/datasets/MONK%27s+Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
