{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "This notebook is an example of how to employ `ortho-svm` to perform regression tasks using **custom kernels** with Support Vector Machines. Althought `scikit-learn` already provides an API for this, `ortho-svm` makes it easier to do as it already has a couple of pre-defined kernels such as:\n",
    "\n",
    "- **Hermite kernel:** A special kernel defined by Hermite polynomials.\n",
    "- **Gegenbauer kernel:** A family of kernels defined by a parameter $\\alpha$ that generalizes most of the classic polynomials, e.g. Legendre and Chebyshev.\n",
    "- **Chebyshev kernel:** A particular instance of the Gegenbauer kernel when $\\alpha = 0$; the Chebyshev polynomials defined are the Chebyshev polynomials of the _first kind_.\n",
    "\n",
    "In this document, an example of a **regression** task will be performed by comparing the results obtained with those from the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "We will be performing regression for the Forest fires dataset [2] using the Gegenbauer kernel defined in `ortho-svm`.\n",
    "\n",
    "The **forest fires** dataset has the following properties:\n",
    "\n",
    "- 517 instances\n",
    "- 12 attributes (categorical and numerical)\n",
    "- 1 prediction attribute\n",
    "\n",
    "The regression problem is a hard one, and we will be comparing closely with the reference paper [1]. In this case, there is no simple way of selecting the hyperparameters, we we will be performing _random search_ for the following parameters:\n",
    "\n",
    "- $C \\in [0, 100]$\n",
    "- $\\varepsilon \\in [0, 1.0]$\n",
    "- $\\alpha \\in (-0.5, 0.5]$\n",
    "- $n \\in [2, 10]$\n",
    "\n",
    "We will be creating probability distributions to sample these values and obtain a good estimate, after all we are performing a proof-of-concept for this kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "Here, we will be performing the following steps, given that we do not have a _test_ dataset:\n",
    "\n",
    "1. Pre-process the data, including loading, cleaning and creating a splitter for the following steps. Split the dataset 80%-20%, and leave the 20% as a _test_ dataset.\n",
    "2. Define a 10-fold cross validation the obtain the best hyper-parameters using the 80% portion of the original dataset.\n",
    "3. Record all the values for the **mean-squared error** and keep track of the best parameters.\n",
    "4. Finally, using the _test_ dataset we predict the needed values and then plot the final results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import uniform, loguniform\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from orthosvm.gramian import gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the _training_ dataset, which is contained in the same directory as this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'Y' 'month' ... 'wind' 'rain' 'area']\n",
      " ['7' '5' 'mar' ... '6.7' '0' '0']\n",
      " ['7' '4' 'oct' ... '0.9' '0' '0']\n",
      " ...\n",
      " ['7' '4' 'aug' ... '6.7' '0' '11.16']\n",
      " ['1' '4' 'aug' ... '4' '0' '0']\n",
      " ['6' '3' 'nov' ... '4.5' '0' '0']]\n",
      "(518, 12)\n"
     ]
    }
   ],
   "source": [
    "# We ignore the last column here because we don't need them, so we\n",
    "# define a list of columns to use\n",
    "dtypes = [\"i4\", \"i4\", \"S3\", \"S3\"]\n",
    "dtypes += 9 * [\"f4\"]\n",
    "raw_data = np.genfromtxt(\"forestfires.csv\", delimiter=\",\", dtype=None, encoding=None)\n",
    "\n",
    "# From this, we extract the labels\n",
    "y = raw_data[:, -1]\n",
    "\n",
    "# We now extract the features\n",
    "xdata = raw_data[:, :-1]\n",
    "\n",
    "# Check number of instances and features\n",
    "print(xdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Perform a 5-fold cross validation\n",
    "\n",
    "We now take the cleaned-up data and perform a grid search for the best possible value for $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to create our estimator, which is a special SVM for classification\n",
    "\n",
    "# We define the function that will compute the Grammian matrix\n",
    "# and we specify the hyper-parameters\n",
    "gram_matrix = gram.gram_matrix(kernel=\"gegenbauer\", degree=2, alpha=0.38)\n",
    "\n",
    "# Next, we pass a dictionary of parameters, in this case the kernel, to the SVC\n",
    "# estimator and we create an object with it\n",
    "params = {\"kernel\": gram_matrix}\n",
    "# params = {\"kernel\": \"rbf\"}\n",
    "svc = SVC(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we proceed to create the GridSearchCV\n",
    "\n",
    "# Create the grid to search\n",
    "c_grid = [{\"C\": [i for i in range(10, 101)]}]\n",
    "# Instantiate the grid search object\n",
    "gcv = GridSearchCV(svc, c_grid, \"accuracy\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train the model\n",
    "\n",
    "We now proceed to train the model using the set-up we have until now, and with the great `scikit-learn` API, after running `GridSearchCV` we obtain the best estimator for the data.\n",
    "\n",
    "**WARNING:** This might take a while, so if you have the processing resources to spare, enable the `n_jobs` parameter to make this easier for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale',\n",
       "                           kernel=<function gram_matrix.<locals>.compute_gram_matrix at 0x7f19364640e0>,\n",
       "                           max_iter=-1, probability=False, random_state=None,\n",
       "                           shrinking=True, tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid=[{'C': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "                                22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "                                34, 35, 36, 37, 38, 39, ...]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We proceed to fit the model to our data\n",
    "# we use the n_jobs parameter before to make up for the time spent here\n",
    "gcv.fit(xdata, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 29}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let us compare the best value found for C\n",
    "gcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are somewhat far from the estimated value for $C$ given in the main reference, but this will suffice for now. We can then use the best estimator found and try out the _test_ dataset with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Test the model\n",
    "\n",
    "We now proceed to test the model, we first load the testing dataset and then we proceed to find the classification accuracy with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.70      0.77       216\n",
      "         1.0       0.75      0.87      0.80       216\n",
      "\n",
      "    accuracy                           0.78       432\n",
      "   macro avg       0.79      0.78      0.78       432\n",
      "weighted avg       0.79      0.78      0.78       432\n",
      "\n",
      "0.7847222222222222\n"
     ]
    }
   ],
   "source": [
    "# We do the same as before to load the information, we can even re-use the same variables\n",
    "raw_data = np.loadtxt(\"monks-1.test\", usecols=[i for i in range(7)])\n",
    "\n",
    "# Again, we extract the labels\n",
    "y = raw_data[:, 0]\n",
    "\n",
    "# We now extract the features\n",
    "xtest = raw_data[:, 1:]\n",
    "\n",
    "# Let us now test our model\n",
    "y_predict = gcv.predict(xtest)\n",
    "# And print out a classification report\n",
    "print(classification_report(y, y_predict))\n",
    "print(accuracy_score(y, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Tian, M., & Wang, W. (2017). Some sets of orthogonal polynomial kernel functions. Applied Soft Computing, 61, 742-756.\n",
    "    \n",
    "2. Forest fires dataset: https://archive.ics.uci.edu/ml/datasets/Forest+Fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
